{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd73c92",
   "metadata": {},
   "source": [
    "# Feature Scaling :\n",
    "> Explaination : IN a data od school report , let say the height of student and the hours study per day is given . as the height will be in hundred's of centimeters and hours per day would be between (0 and 10) then model might think that the height is more important then the study hours per day . so we need to scale the data from 0 to 1 for  the better training.\n",
    "\n",
    "## Way to do it :\n",
    "   1. Standard Scaler : bring mean equal to 0 and standard deviation equal  to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25bd7e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ea628",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar=StandardScaler()\n",
    "X_Scaled=scalar.fit_transform()\n",
    "\n",
    "scalar=MinMaxScaler()\n",
    "X_Scaled=scalar.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a6090ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard scaled output : \n",
      "      Study  TestingScore\n",
      "0 -1.414214     -1.414214\n",
      "1 -0.707107     -0.707107\n",
      "2  0.000000      0.000000\n",
      "3  0.707107      0.707107\n",
      "4  1.414214      1.414214\n"
     ]
    }
   ],
   "source": [
    "data={\n",
    "    \"Study\":[1,2,3,4,5],\n",
    "    \"TestingScore\":[40,50,60,70,80]\n",
    "}\n",
    "\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "scalar=StandardScaler()\n",
    "Standard_Scaled=scalar.fit_transform(df)\n",
    "\n",
    "print(\"Standard scaled output : \")\n",
    "print(pd.DataFrame(Standard_Scaled,columns=[\"Study\",\"TestingScore\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebdce8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Study  TestingScore\n",
      "0   0.00          0.00\n",
      "1   0.25          0.25\n",
      "2   0.50          0.50\n",
      "3   0.75          0.75\n",
      "4   1.00          1.00\n"
     ]
    }
   ],
   "source": [
    "data={\n",
    "    \"Study\":[1,2,3,4,5],\n",
    "    \"TestingScore\":[40,50,60,70,80]\n",
    "}\n",
    "\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "MinMaxscalar=MinMaxScaler()\n",
    "MinMaxX_Scaled=MinMaxscalar.fit_transform(df)\n",
    "print(pd.DataFrame(MinMaxX_Scaled,columns=[\"Study\",\"TestingScore\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fb3fd5",
   "metadata": {},
   "source": [
    "# Feature Scaling: The Complete Guide\n",
    "\n",
    "**Feature Scaling** is a preprocessing step used to standardize the range of independent variables or features of data. In machine learning, it ensures that features with large magnitudes do not dominate those with smaller magnitudes.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Why Scale Your Data?\n",
    "\n",
    "### A. Equalizing Feature Impact\n",
    "Algorithms that rely on **distance calculations** (like Euclidean distance) are highly sensitive to the scale of the input.\n",
    "* **Example:** If \"Income\" ranges from 0 to 1,000,000 and \"Age\" ranges from 0 to 100, the \"Income\" feature will completely dominate the distance calculation.\n",
    "\n",
    "\n",
    "\n",
    "### B. Speeding Up Convergence\n",
    "In models using **Gradient Descent** (Neural Networks, Linear Regression), scaling ensures the cost function has a spherical shape rather than an elongated one. This allows the optimizer to reach the \"global minimum\" much faster.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Main Scaling Techniques\n",
    "\n",
    "### I. Normalization (Min-Max Scaling)\n",
    "Shifts and rescales the data so that it falls within a specific range, usually **[0, 1]**.\n",
    "\n",
    "**Mathematical Formula:**\n",
    "$$X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "* **When to use:** When you don't know the distribution of your data or when you know it is NOT Gaussian (Normal).\n",
    "* **Risk:** High sensitivity to **outliers**.\n",
    "\n",
    "\n",
    "\n",
    "### II. Standardization (Z-Score Normalization)\n",
    "Centers the data such that the mean is **0** and the standard deviation is **1**.\n",
    "\n",
    "**Mathematical Formula:**\n",
    "$$X_{std} = \\frac{X - \\mu}{\\sigma}$$\n",
    "*(where $\\mu$ is the mean and $\\sigma$ is the standard deviation)*\n",
    "\n",
    "* **When to use:** Most common for algorithms like SVM, Logistic Regression, and PCA.\n",
    "* **Benefit:** Much more robust to outliers compared to Normalization.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Algorithm Requirements\n",
    "\n",
    "| Algorithm | Scale Required? | Reason |\n",
    "| :--- | :--- | :--- |\n",
    "| **KNN / SVM / K-Means** | **Yes (Critical)** | Based on distance metrics. |\n",
    "| **Principal Component Analysis (PCA)** | **Yes (Critical)** | PCA seeks to maximize variance. |\n",
    "| **Linear / Logistic Regression** | **Yes** | Faster Gradient Descent. |\n",
    "| **Neural Networks** | **Yes** | Faster training; prevents vanishing gradients. |\n",
    "| **Tree-based (Random Forest, XGB) ** | **No** | Trees split based on value thresholds. |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Python Implementation (Scikit-Learn)\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Using Standardization\n",
    "scaler = StandardScaler()\n",
    "df['Scaled_Feature'] = scaler.fit_transform(df[['Original_Feature']])\n",
    "\n",
    "# Using Normalization\n",
    "minmax = MinMaxScaler()\n",
    "df['Normalized_Feature'] = minmax.fit_transform(df[['Original_Feature']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
