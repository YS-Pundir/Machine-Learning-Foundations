{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd73c92",
   "metadata": {},
   "source": [
    "# Feature Scaling :\n",
    "> Explaination : In a data of school report , let say the height of student and the hours study per day is given . as the height will be in hundred's of centimeters and hours per day would be between (0 and 10) then model might think that the height is more important then the study hours per day . so we need to scale the data from 0 to 1 for  the better training.\n",
    "\n",
    "## Way to do it :\n",
    "   1. Standard Scaler : bring mean equal to 0 and standard deviation equal  to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bd7e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8ea628",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar=StandardScaler()\n",
    "X_Scaled=scalar.fit_transform()\n",
    "\n",
    "scalar=MinMaxScaler()\n",
    "X_Scaled=scalar.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6090ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\n",
    "    \"Study\":[1,2,3,4,5],\n",
    "    \"TestingScore\":[40,50,60,70,80]\n",
    "}\n",
    "\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "scalar=StandardScaler()\n",
    "Standard_Scaled=scalar.fit_transform(df)\n",
    "print(\"Standard scaled data\")\n",
    "print(Standard_Scaled)\n",
    "\n",
    "print(\"Standard scaled output : \")\n",
    "print(pd.DataFrame(Standard_Scaled,columns=[\"Study\",\"TestingScore\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdce8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\n",
    "    \"Study\":[1,2,3,4,5],\n",
    "    \"TestingScore\":[40,50,60,70,80]\n",
    "}\n",
    "\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "MinMaxscalar=MinMaxScaler()\n",
    "MinMaxX_Scaled=MinMaxscalar.fit_transform(df)\n",
    "print(pd.DataFrame(MinMaxX_Scaled,columns=[\"Study\",\"TestingScore\"]))\n",
    "\n",
    "x=df[[\"Study\"]]\n",
    "y=df[[\"TestingScore\"]]\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "print()\n",
    "\n",
    "print(\"Train Data\")\n",
    "\n",
    "print(X_train)\n",
    "\n",
    "print()\n",
    "print(\"Test Data\")\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fb3fd5",
   "metadata": {},
   "source": [
    "# Feature Scaling: The Complete Guide\n",
    "\n",
    "**Feature Scaling** is a preprocessing step used to standardize the range of independent variables or features of data. In machine learning, it ensures that features with large magnitudes do not dominate those with smaller magnitudes.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Why Scale Your Data?\n",
    "\n",
    "### A. Equalizing Feature Impact\n",
    "Algorithms that rely on **distance calculations** (like Euclidean distance) are highly sensitive to the scale of the input.\n",
    "* **Example:** If \"Income\" ranges from 0 to 1,000,000 and \"Age\" ranges from 0 to 100, the \"Income\" feature will completely dominate the distance calculation.\n",
    "\n",
    "\n",
    "\n",
    "### B. Speeding Up Convergence\n",
    "In models using **Gradient Descent** (Neural Networks, Linear Regression), scaling ensures the cost function has a spherical shape rather than an elongated one. This allows the optimizer to reach the \"global minimum\" much faster.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Main Scaling Techniques\n",
    "\n",
    "### I. Normalization (Min-Max Scaling)\n",
    "Shifts and rescales the data so that it falls within a specific range, usually **[0, 1]**.\n",
    "\n",
    "**Mathematical Formula:**\n",
    "$$X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "* **When to use:** When you don't know the distribution of your data or when you know it is NOT Gaussian (Normal).\n",
    "* **Risk:** High sensitivity to **outliers**.\n",
    "\n",
    "\n",
    "\n",
    "### II. Standardization (Z-Score Normalization)\n",
    "Centers the data such that the mean is **0** and the standard deviation is **1**.\n",
    "\n",
    "**Mathematical Formula:**\n",
    "$$X_{std} = \\frac{X - \\mu}{\\sigma}$$\n",
    "*(where $\\mu$ is the mean and $\\sigma$ is the standard deviation)*\n",
    "\n",
    "* **When to use:** Most common for algorithms like SVM, Logistic Regression, and PCA.\n",
    "* **Benefit:** Much more robust to outliers compared to Normalization.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Algorithm Requirements\n",
    "\n",
    "| Algorithm | Scale Required? | Reason |\n",
    "| :--- | :--- | :--- |\n",
    "| **KNN / SVM / K-Means** | **Yes (Critical)** | Based on distance metrics. |\n",
    "| **Principal Component Analysis (PCA)** | **Yes (Critical)** | PCA seeks to maximize variance. |\n",
    "| **Linear / Logistic Regression** | **Yes** | Faster Gradient Descent. |\n",
    "| **Neural Networks** | **Yes** | Faster training; prevents vanishing gradients. |\n",
    "| **Tree-based (Random Forest, XGB) ** | **No** | Trees split based on value thresholds. |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Python Implementation (Scikit-Learn)\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Using Standardization\n",
    "scaler = StandardScaler()\n",
    "df['Scaled_Feature'] = scaler.fit_transform(df[['Original_Feature']])\n",
    "\n",
    "# Using Normalization\n",
    "minmax = MinMaxScaler()\n",
    "df['Normalized_Feature'] = minmax.fit_transform(df[['Original_Feature']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2cf662",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd912c14",
   "metadata": {},
   "source": [
    "# Data Splitting: The \"Practice vs. Exam\" Concept\n",
    "\n",
    "In Machine Learning, **Splitting** is the process of dividing your dataset into separate parts to ensure your model can generalize to new data rather than just memorizing your current data.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The Core Components\n",
    "\n",
    "### üìö The Training Set\n",
    "* **Size:** Usually **70% to 80%** of your total data.\n",
    "* **Purpose:** This is the \"Study Guide.\" The model looks at this data to find patterns, trends, and mathematical relationships.\n",
    "\n",
    "### üìù The Testing Set\n",
    "* **Size:** Usually **20% to 30%** of your total data.\n",
    "* **Purpose:** This is the \"Final Exam.\" This data is hidden from the model during the learning phase. It is used only at the very end to see how well the model performs on data it has never seen before.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why is Splitting Necessary?\n",
    "\n",
    "The main goal of splitting is to avoid **Overfitting**.\n",
    "* **Overfitting:** When a model learns the training data *too* well (including the random noise and errors), causing it to perform poorly in the real world.\n",
    "* **Generalization:** A split allows us to prove that the model has actually learned the logic behind the data, not just memorized the answers.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Python Implementation\n",
    "\n",
    "We use the `train_test_split` function from the `scikit-learn` library.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X = Features (Input variables)\n",
    "# y = Target (The thing you want to predict)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.2,    # 20% for testing, 80% for training\n",
    "    random_state=42   # Ensures the same split every time you run the code\n",
    ")\n",
    "\n",
    "Term,Simple Definition\n",
    "X_train,The input features the model uses to learn.\n",
    "y_train,The correct answers provided to the model during learning.\n",
    "X_test,\"New input features used to test the model's \"\"intelligence.\"\"\"\n",
    "y_test,\"The \"\"Answer Key\"\" used to grade the model's test performance.\"\n",
    "Random State,A seed number that makes your random split reproducible by others.\n",
    "\n",
    "5. The Workflow Summary\n",
    "Collect your data.\n",
    "\n",
    "Clean and Encode your data (Label Encoding/One-Hot).\n",
    "\n",
    "Scale your features (StandardScaler/MinMaxScaler).\n",
    "\n",
    "Split into Train and Test sets.\n",
    "\n",
    "Train your model on the Training set.\n",
    "\n",
    "Evaluate (Grade) your model on the Testing set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
