{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d3b1fc",
   "metadata": {},
   "source": [
    "# üìî ML Lab: Data Preprocessing Workbook\n",
    "**Course:** Software Engineering (ML Specialization)  \n",
    "**Campus:** IIT Roorkee  \n",
    "**Objective:** Mastering the transition from Raw Data to Model-Ready Tensors.\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è 1. The Preprocessing Roadmap\n",
    "In our lectures, we discussed that data must be \"clean\" before it is \"smart.\" Here is your practice schedule:\n",
    "\n",
    "| Phase | Level | Focus Area | Recommended Data Source |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Phase 1** | **Beginner** | Missing Data & Encoding | [Student Placement Dataset](https://www.kaggle.com/datasets/sonalshinde123/student-placement-dataset) |\n",
    "| **Phase 2** | **Intermediate** | Scaling & Outliers | [Adult Census Income](https://www.kaggle.com/datasets/uciml/adult-census-income) |\n",
    "| **Phase 3** | **Advanced** | Pipeline & Leakage | [Titanic Disaster Dataset](https://www.kaggle.com/c/titanic/data) |\n",
    "| **Phase 4** | **Expert** | Temporal Splitting | [AAPL Stock History (yfinance)](https://finance.yahoo.com/quote/AAPL/history) |\n",
    "\n",
    "---\n",
    "\n",
    "## üìê 2. Mathematical Foundations\n",
    "Before coding, remember the math behind the transformation. We use these to ensure no single feature dominates the model due to its scale.\n",
    "\n",
    "### A. Standardization (Z-Score)\n",
    "This centers the data around a mean of 0 with a standard deviation of 1.\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "### B. Min-Max Scaling (Normalization)\n",
    "This squashes all values into a specific range, typically $[0, 1]$.\n",
    "$$x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üíª 3. Essential Implementation (The IIT-R Standard)\n",
    "\n",
    "### Handling Categorical Data\n",
    "Always remember to drop the first dummy variable to avoid the **Dummy Variable Trap** (Multicollinearity).\n",
    "```python\n",
    "import pandas as pd\n",
    "# One-Hot Encoding for Nominal Data\n",
    "df = pd.get_dummies(df, columns=['Branch'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904c7ccc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìç Phase 1: The \"Placement Portal\" Cleaning Task\n",
    "**The Problem:** You are given a dataset of student records. The `CGPA` column has missing values because some students haven't updated their profiles. The `Branch` column contains \"CSE\", \"ECE\", and \"Mech\". The `Internship_Done` column is \"Yes\" or \"No\".\n",
    "\n",
    "**Your Task:**\n",
    "1.  **Imputation:** Check for null values. Fill missing `CGPA` with the **Median** value of the class.\n",
    "2.  **Encoding:** * Convert `Branch` using **One-Hot Encoding** (ensure you avoid the dummy variable trap).\n",
    "    * Convert `Internship_Done` using **Label Encoding** (0 for No, 1 for Yes).\n",
    "3.  **Verification:** Display the first 5 rows to ensure no strings remain.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üìç Phase 2: The \"Census Income\" Scaling Challenge\n",
    "**The Problem:**\n",
    "In a Census dataset, `Age` ranges from 17 to 90, but `Capital_Gain` ranges from 0 to 99,999. If you feed this into a K-Nearest Neighbors (KNN) model, the `Capital_Gain` will dominate the distance calculation, making `Age` irrelevant.\n",
    "\n",
    "**Your Task:**\n",
    "1.  **Outlier Detection:** Use the **IQR (Interquartile Range)** method to find if there are extreme outliers in `Capital_Gain`.\n",
    "2.  **Standardization:** Apply `StandardScaler` to the data so that the mean becomes 0 and the standard deviation becomes 1.\n",
    "3.  **Comparison:** Use a boxplot to visualize the data distribution **before** and **after** scaling.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üìç Phase 3: The \"Titanic\" End-to-End Pipeline\n",
    "**The Problem:**\n",
    "This is a classic \"messy\" real-world scenario. The `Age` is missing in 20% of rows, `Cabin` is missing in 77% of rows, and the `Embarked` port is missing in 2 rows.\n",
    "\n",
    "**Your Task:**\n",
    "1.  **Data Reduction:** Drop the `Cabin` column (too much missing data to be useful).\n",
    "2.  **Smart Imputation:** Fill missing `Age` values using the average age of people with the same \"Title\" (Mr., Mrs., Miss).\n",
    "3.  **The Golden Split:** * Separate the features ($X$) from the target ($y$ - Survived).\n",
    "    * Split into **80% Train** and **20% Test**.\n",
    "4.  **Feature Scaling:** Fit the scaler on the **Training set only** and transform both the training and test sets. \n",
    "    * *Question:* Why didn't we fit on the Test set? (Write your answer in a comment).\n",
    "\n",
    "---\n",
    "\n",
    "## üìç Phase 4: The \"Stock Market\" Temporal Challenge\n",
    "**The Problem:**\n",
    "Stock prices are time-dependent. If you use a random `train_test_split`, you will be using \"future\" prices to predict \"past\" prices, which is impossible in the real world.\n",
    "\n",
    "**Your Task:**\n",
    "1.  **Temporal Split:** Sort the data by `Date`. Use the first 80% of chronological data for Training and the remaining 20% for Testing.\n",
    "2.  **Handling Skewness:** Stock `Volume` is often highly skewed. Apply a **Log Transformation** to make the distribution more Gaussian (Normal).\n",
    "3.  **Final Check:** Ensure your final `X_train` and `X_test` are saved as NumPy arrays, ready for a Linear Regression model.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Final Goal\n",
    "By the end of these phases, you should have a clean, numerical, and scaled NumPy array ($X$) and a target vector ($y$) ready for any Scikit-Learn model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
